From d37fd549d839de5d4b3a9ba9bec89b9037f251fa Mon Sep 17 00:00:00 2001
From: Tahsin Erdogan <tahsin@google.com>
Date: Fri, 8 Jan 2016 10:30:24 -0800
Subject: [PATCH] block: Add block histograms

Adds (read|write)_(dma|request)_histo to block sysfs

requires CONFIG_BLOCK_HISTOGRAM=y

/sys/block/sdX/read_dma_histo - read time
/sys/block/sdX/read_request_histo - read + queue time

writing to /sys/block/sdX/base_histo_* resets the histogram

Change-Id: I467485b90103b016a8d577af2f7fab780bdc2c9b
Signed-off-by: Khazhismel Kumykov <khazhy@google.com>
---
 arch/x86/include/asm/timer.h |  77 +++++
 arch/x86/kernel/tsc.c        |  48 +--
 block/Kconfig                |  35 ++
 block/blk-core.c             |  32 +-
 block/blk-merge.c            |   5 +-
 block/blk-mq.c               |   1 -
 block/cfq-iosched.c          |  16 +-
 block/genhd.c                | 626 ++++++++++++++++++++++++++++++++++-
 block/partition-generic.c    |  37 ++-
 include/linux/blkdev.h       |  17 +-
 include/linux/genhd.h        | 112 ++++++-
 include/linux/time.h         |   5 +
 12 files changed, 933 insertions(+), 78 deletions(-)

diff --git a/arch/x86/include/asm/timer.h b/arch/x86/include/asm/timer.h
index 7365dd4acffb..c1caa1332ece 100644
--- a/arch/x86/include/asm/timer.h
+++ b/arch/x86/include/asm/timer.h
@@ -14,6 +14,7 @@ extern void recalibrate_cpu_khz(void);
 extern int no_timer_check;
 
 extern bool using_native_sched_clock(void);
+extern struct static_key_false __use_tsc; // TODO
 
 /*
  * We use the full linear equation: f(x) = a + b*x, in order to allow
@@ -35,4 +36,80 @@ struct cyc2ns_data {
 extern void cyc2ns_read_begin(struct cyc2ns_data *);
 extern void cyc2ns_read_end(void);
 
+/*
+ * Use a ring-buffer like data structure, where a writer advances the head by
+ * writing a new data entry and a reader advances the tail when it observes a
+ * new entry.
+ *
+ * Writers are made to wait on readers until there's space to write a new
+ * entry.
+ *
+ * This means that we can always use an {offset, mul} pair to compute a ns
+ * value that is 'roughly' in the right direction, even if we're writing a new
+ * {offset, mul} pair during the clock read.
+ *
+ * The down-side is that we can no longer guarantee strict monotonicity anymore
+ * (assuming the TSC was that to begin with), because while we compute the
+ * intersection point of the two clock slopes and make sure the time is
+ * continuous at the point of switching; we can no longer guarantee a reader is
+ * strictly before or after the switch point.
+ *
+ * It does mean a reader no longer needs to disable IRQs in order to avoid
+ * CPU-Freq updates messing with his times, and similarly an NMI reader will
+ * no longer run the risk of hitting half-written state.
+ */
+
+struct cyc2ns {
+	struct cyc2ns_data data[2];	/*  0 + 2*24 = 48 */
+    seqcount_t seq;             /* 32 + 4    = 36 */
+}; /* exactly fits one cacheline */
+
+DECLARE_PER_CPU_ALIGNED(struct cyc2ns, cyc2ns);
+
+#define CYC2NS_SCALE_FACTOR 10 /* 2^10, carefully chosen */
+
+static inline unsigned long long cycles_2_ns(unsigned long long cyc)
+{
+	struct cyc2ns_data data;
+	unsigned long long ns;
+
+	cyc2ns_read_begin(&data);
+
+	ns = data.cyc2ns_offset;
+	ns += mul_u64_u32_shr(cyc, data.cyc2ns_mul, data.cyc2ns_shift);
+
+	cyc2ns_read_end();
+
+	return ns;
+}
+
+/*
+ * Scheduler clock - returns current time in nanosec units. The caller needs to
+ * ensure a reschedule won't happen during the execution of this function, by
+ * disabling preemption, or in some other way.
+ */
+static inline u64 __sched_clock(void)
+{
+	if (static_branch_likely(&__use_tsc)) {
+		u64 tsc_now = rdtsc();
+
+		/* return the value in ns */
+		return cycles_2_ns(tsc_now);
+	}
+
+	/*
+	 * Fall back to jiffies if there's no TSC available:
+	 * ( But note that we still use it if the TSC is marked
+	 *   unstable. We do this because unlike Time Of Day,
+	 *   the scheduler clock tolerates small errors and it's
+	 *   very important for it to be as fast as the platform
+	 *   can achieve it. )
+	 */
+
+	/* No locking but a rare wrong value is not a big deal: */
+	return (jiffies_64 - INITIAL_JIFFIES) * (1000000000 / HZ);
+}
+
+#define CONFIG_INLINE_SCHED_CLOCK
+
 #endif /* _ASM_X86_TIMER_H */
diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index e169e85db434..70e8784c3b22 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -40,9 +40,10 @@ static int __read_mostly tsc_unstable;
 /* native_sched_clock() is called before tsc_init(), so
    we must start with the TSC soft disabled to prevent
    erroneous rdtsc usage on !boot_cpu_has(X86_FEATURE_TSC) processors */
-static int __read_mostly tsc_disabled = -1;
+int __read_mostly tsc_disabled = -1;
+EXPORT_SYMBOL_GPL(tsc_disabled);
 
-static DEFINE_STATIC_KEY_FALSE(__use_tsc);
+DEFINE_STATIC_KEY_FALSE(__use_tsc);
 
 int tsc_clocksource_reliable;
 
@@ -51,13 +52,8 @@ static u32 art_to_tsc_denominator;
 static u64 art_to_tsc_offset;
 struct clocksource *art_related_clocksource;
 
-struct cyc2ns {
-	struct cyc2ns_data data[2];	/*  0 + 2*16 = 32 */
-	seqcount_t	   seq;		/* 32 + 4    = 36 */
-
-}; /* fits one cacheline */
-
-static DEFINE_PER_CPU_ALIGNED(struct cyc2ns, cyc2ns);
+DEFINE_PER_CPU_ALIGNED(struct cyc2ns, cyc2ns);
+EXPORT_SYMBOL_GPL(cyc2ns);
 
 void cyc2ns_read_begin(struct cyc2ns_data *data)
 {
@@ -122,21 +118,6 @@ static void __init cyc2ns_init(int cpu)
 	seqcount_init(&c2n->seq);
 }
 
-static inline unsigned long long cycles_2_ns(unsigned long long cyc)
-{
-	struct cyc2ns_data data;
-	unsigned long long ns;
-
-	cyc2ns_read_begin(&data);
-
-	ns = data.cyc2ns_offset;
-	ns += mul_u64_u32_shr(cyc, data.cyc2ns_mul, data.cyc2ns_shift);
-
-	cyc2ns_read_end();
-
-	return ns;
-}
-
 static void set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long tsc_now)
 {
 	unsigned long long ns_now;
@@ -191,24 +172,7 @@ static void set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long tsc_
  */
 u64 native_sched_clock(void)
 {
-	if (static_branch_likely(&__use_tsc)) {
-		u64 tsc_now = rdtsc();
-
-		/* return the value in ns */
-		return cycles_2_ns(tsc_now);
-	}
-
-	/*
-	 * Fall back to jiffies if there's no TSC available:
-	 * ( But note that we still use it if the TSC is marked
-	 *   unstable. We do this because unlike Time Of Day,
-	 *   the scheduler clock tolerates small errors and it's
-	 *   very important for it to be as fast as the platform
-	 *   can achieve it. )
-	 */
-
-	/* No locking but a rare wrong value is not a big deal: */
-	return (jiffies_64 - INITIAL_JIFFIES) * (1000000000 / HZ);
+	return __sched_clock();
 }
 
 /*
diff --git a/block/Kconfig b/block/Kconfig
index 28ec55752b68..ab271976a886 100644
--- a/block/Kconfig
+++ b/block/Kconfig
@@ -190,6 +190,41 @@ source "block/partitions/Kconfig"
 
 endmenu
 
+config BLOCK_HISTOGRAM
+	bool "Performance histogram data"
+	default n
+	---help---
+	  This option causes block devices to collect statistics on transfer
+	  sizes and times.  Useful for performance-tuning a system.  Creates
+	  entries in /sysfs/block/.
+
+	  If you are unsure, say N here.
+
+config HISTO_SIZE_BUCKETS
+	int "Number of size buckets in histogram"
+	depends on BLOCK_HISTOGRAM
+	default "10"
+	---help---
+	  This option controls how many buckets are used to collect
+	  transfer size statistics.
+
+config HISTO_TIME_BUCKETS
+	int "Number of time buckets in histogram"
+	depends on BLOCK_HISTOGRAM
+	default "11"
+	---help---
+	  This option controls how many buckets are used to collect
+	  transfer time statistics.
+
+config HISTO_SEEK_BUCKETS
+	int "Number of seek buckets in histogram"
+	depends on BLOCK_HISTOGRAM
+	default "28"
+	---help---
+	  This option controls how many buckets are used to collect
+	  disk seek statistics. The actual number of buckets is 1 greater
+	  than the number specified here as the last bucket is a catch-all one.
+
 endif # BLOCK
 
 config BLOCK_COMPAT
diff --git a/block/blk-core.c b/block/blk-core.c
index 3ba4326a63b5..44b3d8544d9b 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -123,7 +123,6 @@ void blk_rq_init(struct request_queue *q, struct request *rq)
 	RB_CLEAR_NODE(&rq->rb_node);
 	rq->tag = -1;
 	rq->internal_tag = -1;
-	rq->start_time = jiffies;
 	set_start_time_ns(rq);
 	rq->part = NULL;
 }
@@ -1566,10 +1565,10 @@ static void add_acct_request(struct request_queue *q, struct request *rq,
 }
 
 static void part_round_stats_single(struct request_queue *q, int cpu,
-				    struct hd_struct *part, unsigned long now,
+				    struct hd_struct *part, unsigned long long now,
 				    unsigned int inflight)
 {
-	if (inflight) {
+	if (inflight & time_after64(now, part->stamp)) {
 		__part_stat_add(cpu, part, time_in_queue,
 				inflight * (now - part->stamp));
 		__part_stat_add(cpu, part, io_ticks, (now - part->stamp));
@@ -1592,12 +1591,12 @@ static void part_round_stats_single(struct request_queue *q, int cpu,
  * second, leading to >100% utilisation.  To deal with that, we call this
  * function to do a round-off before returning the results when reading
  * /proc/diskstats.  This accounts immediately for all queue usage up to
- * the current jiffies and restarts the counters again.
+ * the current ns and restarts the counters again.
  */
 void part_round_stats(struct request_queue *q, int cpu, struct hd_struct *part)
 {
 	struct hd_struct *part2 = NULL;
-	unsigned long now = jiffies;
+	unsigned long long now = sched_clock();
 	unsigned int inflight[2];
 	int stats = 0;
 
@@ -1706,6 +1705,7 @@ bool bio_attempt_back_merge(struct request_queue *q, struct request *req,
 	req->biotail->bi_next = bio;
 	req->biotail = bio;
 	req->__data_len += bio->bi_iter.bi_size;
+	req->__nr_sectors += bio_sectors(bio);
 	req->ioprio = ioprio_best(req->ioprio, bio_prio(bio));
 
 	blk_account_io_start(req, false);
@@ -1730,6 +1730,7 @@ bool bio_attempt_front_merge(struct request_queue *q, struct request *req,
 
 	req->__sector = bio->bi_iter.bi_sector;
 	req->__data_len += bio->bi_iter.bi_size;
+	req->__nr_sectors += bio_sectors(bio);
 	req->ioprio = ioprio_best(req->ioprio, bio_prio(bio));
 
 	blk_account_io_start(req, false);
@@ -2584,7 +2585,7 @@ void blk_account_io_done(struct request *req)
 	 * containing request is enough.
 	 */
 	if (blk_do_io_stat(req) && !(req->rq_flags & RQF_FLUSH_SEQ)) {
-		unsigned long duration = jiffies - req->start_time;
+		unsigned long long now = sched_clock();
 		const int rw = rq_data_dir(req);
 		struct hd_struct *part;
 		int cpu;
@@ -2593,12 +2594,25 @@ void blk_account_io_done(struct request *req)
 		part = req->part;
 
 		part_stat_inc(cpu, part, ios[rw]);
-		part_stat_add(cpu, part, ticks[rw], duration);
+		if (time_after64(now, req->start_time_ns))
+			part_stat_add(cpu, part, ticks[rw],
+					now - req->start_time_ns);
 		part_round_stats(req->q, cpu, part);
+		block_histogram_completion(cpu, part, req, now, 0);
 		part_dec_in_flight(req->q, part, rw);
 
 		hd_struct_put(part);
 		part_stat_unlock();
+	} else if (blk_op_is_scsi(req_op(req)) && req->rq_disk) {
+#ifdef CONFIG_BLOCK_HISTOGRAMS
+		unsigned long long now = sched_clock();
+		struct hd_struct *part;
+		int cpu = part_stat_lock();
+
+		part = &req->rq_disk->part0;
+		block_histogram_completion(cpu, part, req, now, 1);
+		part_stat_unlock();
+#endif
 	}
 }
 
@@ -2822,7 +2836,8 @@ static void blk_dequeue_request(struct request *rq)
 	if (blk_account_rq(rq)) {
 		q->in_flight[rq_is_sync(rq)]++;
 		set_io_start_time_ns(rq);
-	}
+	} else if (blk_op_is_scsi(req_op(rq)) && rq->rq_disk)
+		set_io_start_time_ns(rq);
 }
 
 /**
@@ -3254,6 +3269,7 @@ void blk_rq_bio_prep(struct request_queue *q, struct request *rq,
 
 	rq->__data_len = bio->bi_iter.bi_size;
 	rq->bio = rq->biotail = bio;
+	rq->__nr_sectors = bio_sectors(bio);
 
 	if (bio->bi_disk)
 		rq->rq_disk = bio->bi_disk;
diff --git a/block/blk-merge.c b/block/blk-merge.c
index f5dedd57dff6..22873d4e0554 100644
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@ -708,13 +708,14 @@ static struct request *attempt_merge(struct request_queue *q,
 	 * the merged requests to be the current request
 	 * for accounting purposes.
 	 */
-	if (time_after(req->start_time, next->start_time))
-		req->start_time = next->start_time;
+	if (time_after64(req->start_time_ns, next->start_time_ns))
+		req->start_time_ns = next->start_time_ns;
 
 	req->biotail->bi_next = next->bio;
 	req->biotail = next->biotail;
 
 	req->__data_len += blk_rq_bytes(next);
+	req->__nr_sectors += blk_rq_size(next);
 
 	elv_merge_requests(q, req, next);
 
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 3d3797327491..d668d77c8893 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -303,7 +303,6 @@ static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 	RB_CLEAR_NODE(&rq->rb_node);
 	rq->rq_disk = NULL;
 	rq->part = NULL;
-	rq->start_time = jiffies;
 #ifdef CONFIG_BLK_CGROUP
 	rq->rl = NULL;
 	set_start_time_ns(rq);
diff --git a/block/cfq-iosched.c b/block/cfq-iosched.c
index 9f342ef1ad42..ebfc04ec97fe 100644
--- a/block/cfq-iosched.c
+++ b/block/cfq-iosched.c
@@ -4215,6 +4215,8 @@ static void cfq_completed_request(struct request_queue *q, struct request *rq)
 	struct cfq_data *cfqd = cfqq->cfqd;
 	const int sync = rq_is_sync(rq);
 	u64 now = ktime_get_ns();
+	unsigned long long sched_now = sched_clock();
+	unsigned long long cfq_fifo_expire;
 
 	cfq_log_cfqq(cfqd, cfqq, "complete rqnoidle %d", req_noidle(rq));
 
@@ -4242,16 +4244,10 @@ static void cfq_completed_request(struct request_queue *q, struct request *rq)
 					cfqq_type(cfqq));
 
 		st->ttime.last_end_request = now;
-		/*
-		 * We have to do this check in jiffies since start_time is in
-		 * jiffies and it is not trivial to convert to ns. If
-		 * cfq_fifo_expire[1] ever comes close to 1 jiffie, this test
-		 * will become problematic but so far we are fine (the default
-		 * is 128 ms).
-		 */
-		if (!time_after(rq->start_time +
-				  nsecs_to_jiffies(cfqd->cfq_fifo_expire[1]),
-				jiffies))
+		cfq_fifo_expire = rq->start_time_ns
+		    + (unsigned long long)cfqd->cfq_fifo_expire[1]
+		    * NSEC_PER_MSEC;
+		if (!time_after64(cfq_fifo_expire, sched_now))
 			cfqd->last_delayed_sync = now;
 	}
 
diff --git a/block/genhd.c b/block/genhd.c
index 96a66f671720..bd31cba8bbbb 100644
--- a/block/genhd.c
+++ b/block/genhd.c
@@ -1105,6 +1105,31 @@ static DEVICE_ATTR(stat, S_IRUGO, part_stat_show, NULL);
 static DEVICE_ATTR(inflight, S_IRUGO, part_inflight_show, NULL);
 static DEVICE_ATTR(badblocks, S_IRUGO | S_IWUSR, disk_badblocks_show,
 		disk_badblocks_store);
+#ifdef	CONFIG_BLOCK_HISTOGRAM
+static DEVICE_ATTR(read_request_histo, S_IRUGO | S_IWUSR,
+		part_read_request_histo_show, part_read_histo_clear);
+static DEVICE_ATTR(read_dma_histo, S_IRUGO | S_IWUSR, part_read_dma_histo_show,
+		part_read_histo_clear);
+static DEVICE_ATTR(write_request_histo, S_IRUGO | S_IWUSR,
+		part_write_request_histo_show, part_write_histo_clear);
+static DEVICE_ATTR(write_dma_histo, S_IRUGO | S_IWUSR,
+		part_write_dma_histo_show, part_write_histo_clear);
+static DEVICE_ATTR(seek_histo, S_IRUGO | S_IWUSR,
+		part_seek_histo_show, part_seek_histo_clear);
+static DEVICE_ATTR(management_dma_histo, S_IRUGO | S_IWUSR,
+		part_management_dma_histo_show, part_management_histo_clear);
+static DEVICE_ATTR(management_request_histo, S_IRUGO | S_IWUSR,
+		part_management_request_histo_show,
+		part_management_histo_clear);
+static DEVICE_ATTR(base_histo_size, S_IRUGO | S_IWUSR,
+		part_base_histo_size_show, part_base_histo_size_write);
+static DEVICE_ATTR(base_histo_time, S_IRUGO | S_IWUSR,
+		part_base_histo_time_show, part_base_histo_time_write);
+static DEVICE_ATTR(histo_time_scale, S_IRUGO | S_IWUSR,
+		part_histo_time_scale_show, part_histo_time_scale_write);
+static DEVICE_ATTR(base_histo_seek, S_IRUGO | S_IWUSR,
+		part_base_histo_seek_show, part_base_histo_seek_write);
+#endif
 #ifdef CONFIG_FAIL_MAKE_REQUEST
 static struct device_attribute dev_attr_fail =
 	__ATTR(make-it-fail, S_IRUGO|S_IWUSR, part_fail_show, part_fail_store);
@@ -1128,6 +1153,19 @@ static struct attribute *disk_attrs[] = {
 	&dev_attr_stat.attr,
 	&dev_attr_inflight.attr,
 	&dev_attr_badblocks.attr,
+#ifdef CONFIG_BLOCK_HISTOGRAM
+	&dev_attr_read_request_histo.attr,
+	&dev_attr_read_dma_histo.attr,
+	&dev_attr_write_request_histo.attr,
+	&dev_attr_write_dma_histo.attr,
+	&dev_attr_seek_histo.attr,
+	&dev_attr_management_request_histo.attr,
+	&dev_attr_management_dma_histo.attr,
+	&dev_attr_base_histo_size.attr,
+	&dev_attr_base_histo_time.attr,
+	&dev_attr_histo_time_scale.attr,
+	&dev_attr_base_histo_seek.attr,
+#endif
 #ifdef CONFIG_FAIL_MAKE_REQUEST
 	&dev_attr_fail.attr,
 #endif
@@ -1307,14 +1345,14 @@ static int diskstats_show(struct seq_file *seqf, void *v)
 			   part_stat_read(hd, ios[READ]),
 			   part_stat_read(hd, merges[READ]),
 			   part_stat_read(hd, sectors[READ]),
-			   jiffies_to_msecs(part_stat_read(hd, ticks[READ])),
+			   nsecs_to_msecs(part_stat_read(hd, ticks[READ])),
 			   part_stat_read(hd, ios[WRITE]),
 			   part_stat_read(hd, merges[WRITE]),
 			   part_stat_read(hd, sectors[WRITE]),
-			   jiffies_to_msecs(part_stat_read(hd, ticks[WRITE])),
+			   nsecs_to_msecs(part_stat_read(hd, ticks[WRITE])),
 			   inflight[0],
-			   jiffies_to_msecs(part_stat_read(hd, io_ticks)),
-			   jiffies_to_msecs(part_stat_read(hd, time_in_queue))
+			   nsecs_to_msecs(part_stat_read(hd, io_ticks)),
+			   nsecs_to_msecs(part_stat_read(hd, time_in_queue))
 			);
 	}
 	disk_part_iter_exit(&piter);
@@ -1411,6 +1449,7 @@ struct gendisk *__alloc_disk_node(int minors, int node_id)
 		}
 		ptbl = rcu_dereference_protected(disk->part_tbl, 1);
 		rcu_assign_pointer(ptbl->part[0], &disk->part0);
+		init_part_histo_defaults(&disk->part0);
 
 		/*
 		 * set_capacity() and get_capacity() currently don't use
@@ -1525,6 +1564,585 @@ int invalidate_partition(struct gendisk *disk, int partno)
 
 EXPORT_SYMBOL(invalidate_partition);
 
+#ifdef	CONFIG_BLOCK_HISTOGRAM
+
+/* Smallest transfer size identifiable (in sectors) by histograms. */
+const int base_histo_size = 4;
+/* Smallest transfer time identifiable (in nanoseconds) by histograms. */
+const int base_histo_time = 10 * NSEC_PER_MSEC;
+/* Transfer time scaling factor. Transfer time is measured in nanoseconds
+ * internally. When printing the histogram, buckets are divided by this factor.
+ * So if the unit factor is 1000000, the histogram will print in milliseconds,
+ * and if it is 1000 it will print in microseconds.
+ */
+const int histo_time_scale = NSEC_PER_MSEC;
+/* Smallest seek distance identifiable (in log base 2 sectors). */
+const int base_histo_seek = 3;
+
+typedef void (part_histo_reset) (struct disk_stats *, int);
+
+/*
+ * Clear one per-cpu instance of one channel of I/O histogram
+ */
+static inline void __block_part_histogram_reset(struct disk_stats *stats,
+						int cmd)
+{
+	if (cmd == READ)
+		memset(&stats->rd_histo, 0, sizeof(stats->rd_histo));
+	else
+		memset(&stats->wr_histo, 0, sizeof(stats->wr_histo));
+}
+
+static inline void __block_part_seek_histogram_reset(struct disk_stats *stats,
+						     int dummy)
+{
+	memset(&stats->seek_histo, 0, sizeof(stats->seek_histo));
+}
+
+static inline void __block_part_management_histogram_reset(
+	struct disk_stats *stats, int dummy)
+{
+	memset(&stats->management_histo, 0, sizeof(stats->management_histo));
+}
+
+/*
+ * Clear one channel of I/O histogram
+ */
+static void block_part_histogram_reset(struct hd_struct *part,
+					part_histo_reset *reset_fn, int cmd)
+{
+#ifdef	CONFIG_SMP
+	int i;
+
+	part_stat_lock();
+	for_each_possible_cpu(i)
+		reset_fn(per_cpu_ptr(part->dkstats, i), cmd);
+#else
+	part_stat_lock();
+	reset_fn(&part.dkstats, cmd);
+#endif
+	part_stat_unlock();
+}
+
+/*
+ * Iterate though all parts of the disk and clear the specified channel of the
+ * histogram.
+ */
+static int block_disk_histogram_reset(struct hd_struct *part,
+					part_histo_reset *reset_fn, int cmd)
+{
+	struct disk_part_iter piter;
+	struct gendisk *disk = part_to_disk(part);
+	struct hd_struct *temp;
+
+	if (!disk)
+		return -ENODEV;
+
+	disk_part_iter_init(&piter, disk, DISK_PITER_INCL_EMPTY_PART0);
+	while ((temp = disk_part_iter_next(&piter)))
+		block_part_histogram_reset(temp, reset_fn, cmd);
+
+	disk_part_iter_exit(&piter);
+	return 0;
+}
+
+void init_part_histo_defaults(struct hd_struct *part)
+{
+	part->last_end_sector = part->start_sect;
+	part->base_histo_size = base_histo_size;
+	part->base_histo_time = base_histo_time;
+	part->histo_time_scale = histo_time_scale;
+	part->base_histo_seek = base_histo_seek;
+}
+
+/*
+ * Map transfer time to histogram bucket. This also uses an exponential
+ * increment, but we like the 1,2,5,10,20,50 progression. Transfer time
+ * is measured in nanoseconds.
+ */
+inline int stats_time_bucket(unsigned long long nanos,
+			     int base_histo_time)
+{
+	int i;
+	unsigned long long t = base_histo_time;
+
+	for (i = 0;; t *= 10) {
+		if (++i >= CONFIG_HISTO_TIME_BUCKETS || nanos <= t)
+			return i - 1;
+		if (++i >= CONFIG_HISTO_TIME_BUCKETS || nanos <= t*2)
+			return i - 1;
+		if (++i >= CONFIG_HISTO_TIME_BUCKETS || nanos <= t*5)
+			return i - 1;
+	}
+}
+
+/*
+ * Map seek distance to histogram bucket. This also uses an exponential
+ * increment : 8, 16, 32, ... sectors.
+ */
+static inline int stats_seek_bucket(sector_t distance, int base_histo_seek)
+{
+	return min(fls64(distance >> base_histo_seek),
+			CONFIG_HISTO_SEEK_BUCKETS);
+}
+
+/*
+ * Map transfer size to histogram bucket.  Transfer sizes are exponentially
+ * increasing. For example: 4, 8, 16, ... sectors.
+ */
+int stats_size_bucket(sector_t sectors, int base_histo_size)
+{
+	int i;
+	/* To make sure bucket for x bytes captures all IOs <= x bytes. */
+	--sectors;
+	WARN_ON(!base_histo_size);
+	do_div(sectors, base_histo_size);
+	if (sectors >= (1 << (CONFIG_HISTO_SIZE_BUCKETS - 2)))
+		return CONFIG_HISTO_SIZE_BUCKETS - 1;
+
+	for (i = 0; sectors > 0; ++i, sectors /= 2)
+		;
+	return i;
+}
+
+/*
+ * Update {read,write}_{dma,request}_histo and seek_histo.
+ *
+ * @part:       disk device partition
+ * @sectors:    request size
+ * @end_sector: end sector of request, for seek histogram. The flash driver
+ *              passes 0, which is a special value to skip updating the seek
+ *              histogram.
+ * @write:      0 if request is a read, anything else if it's a write
+ * @nanos:      total request time
+ * @nanos_dma:  I/O service time
+ *
+ * Time is measured in tens of microseconds when this function is called by
+ * the flash driver. Otherwise it is in milliseconds.
+ */
+void __block_histogram_completion(int cpu,
+				  struct hd_struct *part,
+				  sector_t sectors,
+				  sector_t end_sector,
+				  int write,
+				  unsigned long long nanos,
+				  unsigned long long nanos_dma)
+{
+	sector_t distance, start_sector;
+	int size_idx, req_time_idx, dma_time_idx, seek_idx;
+
+	if (end_sector > 0) {
+		start_sector = end_sector - sectors;
+		if (start_sector >= part->last_end_sector)
+			distance = start_sector - part->last_end_sector;
+		else
+			distance = part->last_end_sector - start_sector;
+
+		seek_idx = stats_seek_bucket(distance, part->base_histo_seek);
+		part_stat_inc(cpu, part, seek_histo[seek_idx]);
+		part->last_end_sector = end_sector;
+	}
+
+	size_idx = stats_size_bucket(sectors, part->base_histo_size);
+	req_time_idx = stats_time_bucket(nanos, part->base_histo_time);
+	if (write)
+		part_stat_inc(cpu, part,
+			wr_histo[HISTO_REQUEST][size_idx][req_time_idx]);
+	else
+		part_stat_inc(cpu, part,
+			rd_histo[HISTO_REQUEST][size_idx][req_time_idx]);
+
+	dma_time_idx = stats_time_bucket(nanos_dma, part->base_histo_time);
+	if (write)
+		part_stat_inc(cpu, part,
+			wr_histo[HISTO_DMA][size_idx][dma_time_idx]);
+	else
+		part_stat_inc(cpu, part,
+			rd_histo[HISTO_DMA][size_idx][dma_time_idx]);
+}
+EXPORT_SYMBOL_GPL(__block_histogram_completion);
+
+/*
+ * Update management_{request,dma}_histo.
+ *
+ * @part:       disk device partition
+ * @nanos:       total request time
+ * @nanos_dma:   I/O service time
+ *
+ * Time is in per-device units; milliseconds for disks and microseconds for
+ * elephants.
+ */
+void __management_histogram_completion(int cpu,
+				       struct hd_struct *part,
+				       unsigned long long nanos,
+				       unsigned long long nanos_dma)
+{
+	int req_time_idx, dma_time_idx;
+
+	req_time_idx = stats_time_bucket(nanos, part->base_histo_time);
+	part_stat_inc(cpu, part, management_histo[HISTO_REQUEST][req_time_idx]);
+	dma_time_idx = stats_time_bucket(nanos_dma, part->base_histo_time);
+	part_stat_inc(cpu, part, management_histo[HISTO_DMA][dma_time_idx]);
+}
+EXPORT_SYMBOL_GPL(__management_histogram_completion);
+
+/*
+ * Helper function: Calls block_histogram_completion after a dma interrupt.
+ * Should be called between part_stat_lock() and part_stat_unlock() calls.
+ */
+void block_histogram_completion(int cpu, struct hd_struct *part,
+		struct request *req, unsigned long long now,
+		int management)
+{
+	static int count = 20000;
+
+	if (req->io_start_time_ns == 0) {
+		if (!(req->cmd_flags & RQF_SOFTBARRIER) && count > 0) {
+			pr_debug("%s: unexpected transfer w/out start time\n",
+				part_to_disk(part)->disk_name);
+			--count;
+		}
+	} else {
+		unsigned long long rq_elapsed = 0, io_elapsed = 0;
+
+		if (time_after64(now, req->start_time_ns))
+			rq_elapsed = now - req->start_time_ns;
+		if (time_after64(now, req->io_start_time_ns))
+			io_elapsed = now - req->io_start_time_ns;
+		if (likely(!management)) {
+			__block_histogram_completion(cpu, part,
+						blk_rq_size(req),
+						blk_rq_pos(req),
+						(op_is_write(req_op(req))),
+						rq_elapsed,
+						io_elapsed);
+		} else {
+			__management_histogram_completion(cpu, part,
+							  rq_elapsed,
+							  io_elapsed);
+		}
+	}
+}
+
+/*
+ * Tiny helper utility, append sprintf() output to a buffer, update pointers,
+ * and guard against overflow.
+ */
+static inline void sysfs_out(char **ptr, ssize_t *rem, const char *fmt, ...)
+{
+	va_list	ap;
+	int	i;
+
+	va_start(ap, fmt);
+	i = vsnprintf(*ptr, *rem, fmt, ap);
+	*ptr += i;
+	*rem -= i;
+	va_end(ap);
+}
+
+/*
+ * For some reason, part_stat_read() doesn't work well using
+ * rd_histo[type][i][j] or seek_histo[i] as the second argument.
+ */
+#ifdef CONFIG_SMP
+
+#define histo_read_1d(part, histo, i)					\
+({									\
+	int cpu, res = 0;						\
+	for_each_possible_cpu(cpu)					\
+		res += per_cpu_ptr(part->dkstats, cpu)->histo[i];	\
+	res;								\
+})
+
+#define histo_read_2d(part, histo, i, j)				\
+({									\
+	int cpu, res = 0;						\
+	for_each_possible_cpu(cpu)					\
+		res += per_cpu_ptr(part->dkstats, cpu)->histo[i][j];	\
+	res;								\
+})
+
+#define histo_read_3d(part, histo, i, j, k)				\
+({									\
+	int cpu, res = 0;						\
+	for_each_possible_cpu(cpu)					\
+		res += per_cpu_ptr(part->dkstats, cpu)->histo[i][j][k];	\
+	res;								\
+})
+
+#else /* !CONFIG_SMP */
+
+#define histo_read_1d(part, histo, i)		(part->dkstats.histo[i])
+#define histo_read_2d(part, histo, i, j)	(part->dkstats.histo[i][j])
+#define histo_read_3d(part, histo, i, j, k)	(part->dkstats.histo[i][j][k])
+
+#endif
+
+static const char *histo_time_string(struct hd_struct *part)
+{
+	switch (part->histo_time_scale) {
+	case NSEC_PER_MSEC:
+		return "ms";
+	case NSEC_PER_USEC:
+		return "us";
+	default:
+		return "unknown";
+	}
+}
+
+/*
+ * Dumps the specified 'type' of histogram for part to out.
+ * The result must be less than PAGE_SIZE.
+ */
+static int dump_histo(struct hd_struct *part, int cmd, int type, char *page)
+{
+	ssize_t	rem = PAGE_SIZE;
+	char *optr = page;
+	int i, j, len, value, size = part->base_histo_size * 512;
+	unsigned long long nanos;
+	static const int mults[3] = {1, 2, 5};
+
+	/*
+	 * Documentation/filesystems/sysfs.txt strongly discourages the use of
+	 * any kind of fancy formatting here. We *are* emitting an array, so
+	 * there needs to be some amount of formatting.
+	 */
+
+	/* Key */
+	len = snprintf(page, rem, "rows = bytes columns = %s\n",
+			histo_time_string(part));
+	page += len;
+	rem -= len;
+
+	/* Row header */
+	len = snprintf(page, rem, "       ");
+	page += len;
+	rem -= len;
+	for (i = 0, nanos = part->base_histo_time;
+	     i < CONFIG_HISTO_TIME_BUCKETS;
+	     nanos *= 10) {
+		for (j = 0; j < 3 && i < CONFIG_HISTO_TIME_BUCKETS; ++j, ++i) {
+			len = snprintf(page, rem, "\t%llu",
+				       nanos * mults[j] /
+				       part->histo_time_scale);
+			page += len;
+			rem -= len;
+		}
+	}
+	len = snprintf(page, rem, "\n");
+	page += len;
+	rem -= len;
+
+	/* Payload */
+	for (i = 0; i < CONFIG_HISTO_SIZE_BUCKETS; i++, size *= 2) {
+		len = snprintf(page, rem, "%7d", size);
+		page += len;
+		rem -= len;
+		for (j = 0; j < CONFIG_HISTO_TIME_BUCKETS; j++) {
+			value = (cmd == READ) ?
+				histo_read_3d(part, rd_histo, type, i, j) :
+				histo_read_3d(part, wr_histo, type, i, j);
+			len = snprintf(page, rem, "\t%d", value);
+			page += len;
+			rem -= len;
+		}
+		len = snprintf(page, rem, "\n");
+		page += len;
+		rem -= len;
+	}
+	return page - optr;
+}
+
+/*
+ * Dumps the seek histogram for part. The result must be less than PAGE_SIZE.
+ */
+static int dump_seek_histo(struct hd_struct *part, char *page)
+{
+	ssize_t rem = PAGE_SIZE;
+	char *optr = page;
+	int i, len;
+
+	len = snprintf(page, rem, "sectors\tnumber\n");
+	page += len;
+	rem -= len;
+
+	for (i = 0; i < CONFIG_HISTO_SEEK_BUCKETS + 1; i++) {
+		if (i < CONFIG_HISTO_SEEK_BUCKETS)
+			len = snprintf(page, rem, "%ld\t%d\n",
+				1UL << (i + part->base_histo_seek),
+				histo_read_1d(part, seek_histo, i));
+		else
+			len = snprintf(page, rem, "inf\t%d\n",
+					histo_read_1d(part, seek_histo, i));
+		page += len;
+		rem -= len;
+	}
+	return page - optr;
+}
+
+/*
+ * Dumps the management command histogram for part. The result must be less than
+ * PAGE_SIZE.
+ */
+static int dump_management_histo(struct hd_struct *part, int type, char *page)
+{
+	ssize_t rem = PAGE_SIZE;
+	char *optr = page;
+	int i, j, len, value;
+	unsigned long long nanos;
+	static const int mults[3] = {1, 2, 5};
+
+	len = snprintf(page, rem, "%s\tnumber\n", histo_time_string(part));
+	page += len;
+	rem -= len;
+
+	for (i = 0, nanos = part->base_histo_time;
+	     i < CONFIG_HISTO_TIME_BUCKETS;
+	     nanos *= 10) {
+		for (j = 0; j < 3 && i < CONFIG_HISTO_TIME_BUCKETS; ++j, ++i) {
+			value = histo_read_2d(part, management_histo, type, i);
+			len = snprintf(page, rem, "%lld\t%d\n",
+			       nanos * mults[j] / part->histo_time_scale,
+			       value);
+			page += len;
+			rem -= len;
+		}
+	}
+
+	return page - optr;
+}
+
+/*
+ * sysfs show() methods for the seven histogram channels.
+ */
+ssize_t part_read_request_histo_show(struct device *dev,
+			struct device_attribute *attr, char *page)
+{
+	return dump_histo(dev_to_part(dev), READ, HISTO_REQUEST, page);
+}
+
+ssize_t part_read_dma_histo_show(struct device *dev,
+			struct device_attribute *attr, char *page)
+{
+	return dump_histo(dev_to_part(dev), READ, HISTO_DMA, page);
+}
+
+ssize_t part_write_request_histo_show(struct device *dev,
+			struct device_attribute *attr, char *page)
+{
+	return dump_histo(dev_to_part(dev), WRITE, HISTO_REQUEST, page);
+}
+
+ssize_t part_write_dma_histo_show(struct device *dev,
+			struct device_attribute *attr, char *page)
+{
+	return dump_histo(dev_to_part(dev), WRITE, HISTO_DMA, page);
+}
+
+ssize_t part_seek_histo_show(struct device *dev,
+			struct device_attribute *attr, char *page)
+{
+	return dump_seek_histo(dev_to_part(dev), page);
+}
+
+ssize_t part_management_request_histo_show(struct device *dev,
+			struct device_attribute *attr, char *page)
+{
+	return dump_management_histo(dev_to_part(dev), HISTO_REQUEST, page);
+}
+
+ssize_t part_management_dma_histo_show(struct device *dev,
+			struct device_attribute *attr, char *page)
+{
+	return dump_management_histo(dev_to_part(dev), HISTO_DMA, page);
+}
+
+/*
+ * Reinitializes the read histograms to 0.
+ */
+ssize_t part_read_histo_clear(struct device *dev,
+		struct device_attribute *attr, const char *page, size_t count)
+{
+	/* Ignore the data, just clear the histogram */
+	int retval = block_disk_histogram_reset(dev_to_part(dev),
+					__block_part_histogram_reset, READ);
+	return (retval == 0 ? count : retval);
+}
+
+/*
+ * Reinitializes the write histograms to 0.
+ */
+ssize_t part_write_histo_clear(struct device *dev,
+		struct device_attribute *attr, const char *page, size_t count)
+{
+	int retval = block_disk_histogram_reset(dev_to_part(dev),
+					__block_part_histogram_reset, WRITE);
+	return (retval == 0 ? count : retval);
+}
+
+/*
+ * Reinitializes the seek histograms to 0.
+ */
+ssize_t part_seek_histo_clear(struct device *dev,
+		struct device_attribute *attr, const char *page, size_t count)
+{
+	int retval = block_disk_histogram_reset(dev_to_part(dev),
+				__block_part_seek_histogram_reset, 0);
+	return (retval == 0 ? count : retval);
+}
+
+/*
+ * Reinitializes the management command histograms to 0.
+ */
+ssize_t part_management_histo_clear(struct device *dev,
+		struct device_attribute *attr, const char *page, size_t count)
+{
+	int retval = block_disk_histogram_reset(dev_to_part(dev),
+				__block_part_management_histogram_reset, 0);
+	return (retval == 0 ? count : retval);
+}
+
+#define SHOW_BASE_FUNCTION(__VAR)					\
+ssize_t part_##__VAR##_show(struct device *dev,				\
+			struct device_attribute *attr, char *page)	\
+{									\
+	struct hd_struct *part = dev_to_part(dev);			\
+									\
+	return sprintf(page, "%d\n", part->__VAR);			\
+}
+
+SHOW_BASE_FUNCTION(base_histo_size);
+SHOW_BASE_FUNCTION(base_histo_time);
+SHOW_BASE_FUNCTION(histo_time_scale);
+SHOW_BASE_FUNCTION(base_histo_seek);
+#undef SHOW_BASE_FUNCTION
+
+#define WRITE_BASE_FUNCTION(__VAR, MIN, reset_fn)			\
+ssize_t part_##__VAR##_write(struct device *dev,			\
+	struct device_attribute *attr, const char *page, size_t count)	\
+{									\
+	struct hd_struct *part = dev_to_part(dev);			\
+	char *p = (char *)page;						\
+	unsigned long __data;						\
+	int ret = kstrtoul(p, 10, &__data);				\
+									\
+	if (ret)							\
+		return ret;						\
+	part->__VAR = max_t(unsigned long, __data, MIN);		\
+	block_disk_histogram_reset(part, reset_fn, READ);		\
+	block_disk_histogram_reset(part, reset_fn, WRITE);		\
+	return count;							\
+}
+
+WRITE_BASE_FUNCTION(base_histo_size, 1, __block_part_histogram_reset);
+WRITE_BASE_FUNCTION(base_histo_time, 1, __block_part_histogram_reset);
+WRITE_BASE_FUNCTION(histo_time_scale, 1, __block_part_histogram_reset);
+WRITE_BASE_FUNCTION(base_histo_seek, 1, __block_part_seek_histogram_reset);
+#undef WRITE_BASE_FUNCTION
+
+#endif
+
+
 /*
  * Disk events - monitor disk events like media change and eject request.
  */
diff --git a/block/partition-generic.c b/block/partition-generic.c
index 91622db9aedf..987f48c29560 100644
--- a/block/partition-generic.c
+++ b/block/partition-generic.c
@@ -129,14 +129,14 @@ ssize_t part_stat_show(struct device *dev,
 		part_stat_read(p, ios[READ]),
 		part_stat_read(p, merges[READ]),
 		(unsigned long long)part_stat_read(p, sectors[READ]),
-		jiffies_to_msecs(part_stat_read(p, ticks[READ])),
+		nsecs_to_msecs(part_stat_read(p, ticks[READ])),
 		part_stat_read(p, ios[WRITE]),
 		part_stat_read(p, merges[WRITE]),
 		(unsigned long long)part_stat_read(p, sectors[WRITE]),
-		jiffies_to_msecs(part_stat_read(p, ticks[WRITE])),
+		nsecs_to_msecs(part_stat_read(p, ticks[WRITE])),
 		inflight[0],
-		jiffies_to_msecs(part_stat_read(p, io_ticks)),
-		jiffies_to_msecs(part_stat_read(p, time_in_queue)));
+		nsecs_to_msecs(part_stat_read(p, io_ticks)),
+		nsecs_to_msecs(part_stat_read(p, time_in_queue)));
 }
 
 ssize_t part_inflight_show(struct device *dev,
@@ -180,6 +180,24 @@ static DEVICE_ATTR(discard_alignment, S_IRUGO, part_discard_alignment_show,
 		   NULL);
 static DEVICE_ATTR(stat, S_IRUGO, part_stat_show, NULL);
 static DEVICE_ATTR(inflight, S_IRUGO, part_inflight_show, NULL);
+#ifdef CONFIG_BLOCK_HISTOGRAM
+static DEVICE_ATTR(read_request_histo, S_IRUGO | S_IWUSR,
+		part_read_request_histo_show, part_read_histo_clear);
+static DEVICE_ATTR(read_dma_histo, S_IRUGO | S_IWUSR, part_read_dma_histo_show,
+		part_read_histo_clear);
+static DEVICE_ATTR(write_request_histo, S_IRUGO | S_IWUSR,
+		part_write_request_histo_show, part_write_histo_clear);
+static DEVICE_ATTR(write_dma_histo, S_IRUGO | S_IWUSR,
+		part_write_dma_histo_show, part_write_histo_clear);
+static DEVICE_ATTR(seek_histo, S_IRUGO | S_IWUSR,
+		part_seek_histo_show, part_seek_histo_clear);
+static DEVICE_ATTR(base_histo_size, S_IRUGO | S_IWUSR,
+		part_base_histo_size_show, part_base_histo_size_write);
+static DEVICE_ATTR(base_histo_time, S_IRUGO | S_IWUSR,
+		part_base_histo_time_show, part_base_histo_time_write);
+static DEVICE_ATTR(base_histo_seek, S_IRUGO | S_IWUSR,
+		part_base_histo_seek_show, part_base_histo_seek_write);
+#endif
 #ifdef CONFIG_FAIL_MAKE_REQUEST
 static struct device_attribute dev_attr_fail =
 	__ATTR(make-it-fail, S_IRUGO|S_IWUSR, part_fail_show, part_fail_store);
@@ -194,6 +212,16 @@ static struct attribute *part_attrs[] = {
 	&dev_attr_discard_alignment.attr,
 	&dev_attr_stat.attr,
 	&dev_attr_inflight.attr,
+#ifdef CONFIG_BLOCK_HISTOGRAM
+	&dev_attr_read_request_histo.attr,
+	&dev_attr_read_dma_histo.attr,
+	&dev_attr_write_request_histo.attr,
+	&dev_attr_write_dma_histo.attr,
+	&dev_attr_seek_histo.attr,
+	&dev_attr_base_histo_size.attr,
+	&dev_attr_base_histo_time.attr,
+	&dev_attr_base_histo_seek.attr,
+#endif
 #ifdef CONFIG_FAIL_MAKE_REQUEST
 	&dev_attr_fail.attr,
 #endif
@@ -330,6 +358,7 @@ struct hd_struct *add_partition(struct gendisk *disk, int partno,
 	p->nr_sects = len;
 	p->partno = partno;
 	p->policy = get_disk_ro(disk);
+	init_part_histo_defaults(p);
 
 	if (info) {
 		struct partition_meta_info *pinfo = alloc_part_info(disk);
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 0ce8a372d506..5040d92e0501 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -150,7 +150,8 @@ struct request {
 
 	unsigned long atomic_flags;
 
-	/* the following two fields are internal, NEVER access directly */
+	/* the following fields are internal, NEVER access directly */
+	sector_t __nr_sectors;		/* Total number of sectors */
 	unsigned int __data_len;	/* total data len */
 	int tag;
 	sector_t __sector;		/* sector cursor */
@@ -205,10 +206,10 @@ struct request {
 	struct hd_struct *part;
 	unsigned long start_time;
 	struct blk_issue_stat issue_stat;
-#ifdef CONFIG_BLK_CGROUP
-	struct request_list *rl;		/* rl this rq is alloced from */
 	unsigned long long start_time_ns;
 	unsigned long long io_start_time_ns;    /* when passed to hardware */
+#ifdef CONFIG_BLK_CGROUP
+	struct request_list *rl;		/* rl this rq is alloced from */
 #endif
 	/* Number of scatter-gather DMA addr+len pairs after
 	 * physical address coalescing is performed.
@@ -1013,6 +1014,7 @@ static inline struct request_queue *bdev_get_queue(struct block_device *bdev)
 
 /*
  * blk_rq_pos()			: the current sector
+ * blk_rq_size()		: the size of the request in sectors
  * blk_rq_bytes()		: bytes left in the entire request
  * blk_rq_cur_bytes()		: bytes left in the current segment
  * blk_rq_err_bytes()		: bytes left till the next error boundary
@@ -1024,6 +1026,11 @@ static inline sector_t blk_rq_pos(const struct request *rq)
 	return rq->__sector;
 }
 
+static inline sector_t blk_rq_size(const struct request *rq)
+{
+	return rq->__nr_sectors;
+}
+
 static inline unsigned int blk_rq_bytes(const struct request *rq)
 {
 	return rq->__data_len;
@@ -1735,7 +1742,6 @@ int kblockd_schedule_delayed_work(struct delayed_work *dwork, unsigned long dela
 int kblockd_schedule_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned long delay);
 int kblockd_mod_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned long delay);
 
-#ifdef CONFIG_BLK_CGROUP
 /*
  * This should not be using sched_clock(). A real patch is in progress
  * to fix this up, until that is in place we need to disable preemption
@@ -1755,6 +1761,7 @@ static inline void set_io_start_time_ns(struct request *req)
 	preempt_enable();
 }
 
+#if defined(CONFIG_BLK_CGROUP) || defined(CONFIG_BLOCK_HISTOGRAM)
 static inline uint64_t rq_start_time_ns(struct request *req)
 {
         return req->start_time_ns;
@@ -1765,8 +1772,6 @@ static inline uint64_t rq_io_start_time_ns(struct request *req)
         return req->io_start_time_ns;
 }
 #else
-static inline void set_start_time_ns(struct request *req) {}
-static inline void set_io_start_time_ns(struct request *req) {}
 static inline uint64_t rq_start_time_ns(struct request *req)
 {
 	return 0;
diff --git a/include/linux/genhd.h b/include/linux/genhd.h
index 5144ebe046c9..886666a26fdf 100644
--- a/include/linux/genhd.h
+++ b/include/linux/genhd.h
@@ -68,6 +68,8 @@ enum {
 #include <linux/fs.h>
 #include <linux/workqueue.h>
 
+struct request;
+
 struct partition {
 	unsigned char boot_ind;		/* 0x80 - active */
 	unsigned char head;		/* starting head */
@@ -81,6 +83,10 @@ struct partition {
 	__le32 nr_sects;		/* nr of sectors in partition */
 } __attribute__((packed));
 
+/* Index into the histo arrays */
+#define HISTO_REQUEST   0
+#define HISTO_DMA       1
+
 struct disk_stats {
 	unsigned long sectors[2];	/* READs and WRITEs */
 	unsigned long ios[2];
@@ -88,6 +94,32 @@ struct disk_stats {
 	unsigned long ticks[2];
 	unsigned long io_ticks;
 	unsigned long time_in_queue;
+#ifdef CONFIG_BLOCK_HISTOGRAM
+	/*
+	 * The block_histogram code implements a 2-variable histogram, with
+	 * transfers tracked by transfer size and completion time. The /sysfs
+	 * files are
+	 * /sysfs/block/DEV/PART/read_request_histo,
+	 * /sysfs/block/DEV/PART/write_request_histo,
+	 * /sysfs/block/DEV/PART/read_dma_histo,
+	 * /sysfs/block/DEV/PART/write_dma_histo,
+	 * /sysfs/block/DEV/PART/seek_histo and the
+	 * /sysfs/block/DEV counterparts.
+	 *
+	 * The *request_histo files measure time from when the request is first
+	 * submitted into the drive's queue.  The *dma_histo files measure time
+	 * from when the request is transferred from the queue to the device.
+	 *
+	 * Management requests are tracked by completion time only. The /sysfs
+	 * files are
+	 * /sysfs/block/DEV/PART/management_request_histo,
+	 * /sysfs/block/DEV/PART/management_dma_histo
+	 */
+	int rd_histo[2][CONFIG_HISTO_SIZE_BUCKETS][CONFIG_HISTO_TIME_BUCKETS];
+	int wr_histo[2][CONFIG_HISTO_SIZE_BUCKETS][CONFIG_HISTO_TIME_BUCKETS];
+	int seek_histo[CONFIG_HISTO_SEEK_BUCKETS + 1];
+	int management_histo[2][CONFIG_HISTO_TIME_BUCKETS];
+#endif
 };
 
 #define PARTITION_META_INFO_VOLNAMELTH	64
@@ -120,12 +152,19 @@ struct hd_struct {
 #ifdef CONFIG_FAIL_MAKE_REQUEST
 	int make_it_fail;
 #endif
-	unsigned long stamp;
+	unsigned long long stamp;
 	atomic_t in_flight[2];
 #ifdef	CONFIG_SMP
 	struct disk_stats __percpu *dkstats;
 #else
 	struct disk_stats dkstats;
+#endif
+#ifdef CONFIG_BLOCK_HISTOGRAM
+	sector_t last_end_sector;
+	int base_histo_size;
+	int base_histo_time;
+	int histo_time_scale;
+	int base_histo_seek;
 #endif
 	struct percpu_ref ref;
 	struct rcu_head rcu_head;
@@ -413,6 +452,77 @@ extern void disk_unblock_events(struct gendisk *disk);
 extern void disk_flush_events(struct gendisk *disk, unsigned int mask);
 extern unsigned int disk_clear_events(struct gendisk *disk, unsigned int mask);
 
+#ifdef	CONFIG_BLOCK_HISTOGRAM
+extern const int base_histo_size;
+extern const int base_histo_time;
+extern const int histo_time_scale;
+extern const int base_histo_seek;
+
+extern void block_histogram_completion(int cpu, struct hd_struct *part,
+			struct request *req, unsigned long long now,
+			int management);
+extern void __block_histogram_completion(int cpu,
+					 struct hd_struct *part,
+					 sector_t sectors,
+					 sector_t end_sector,
+					 int write,
+					 unsigned long long nanos,
+					 unsigned long long nanos_dma);
+void __management_histogram_completion(int cpu,
+				       struct hd_struct *part,
+				       unsigned long long time,
+				       unsigned long long time_dma);
+extern ssize_t part_read_request_histo_show(struct device *dev,
+			struct device_attribute *attr, char *page);
+extern ssize_t part_read_dma_histo_show(struct device *dev,
+			struct device_attribute *attr, char *page);
+extern ssize_t part_write_request_histo_show(struct device *dev,
+			struct device_attribute *attr, char *page);
+extern ssize_t part_write_dma_histo_show(struct device *dev,
+			struct device_attribute *attr, char *page);
+extern ssize_t part_write_dma_histo_show(struct device *dev,
+			struct device_attribute *attr, char *page);
+extern ssize_t part_seek_histo_show(struct device *dev,
+			struct device_attribute *attr, char *page);
+extern ssize_t part_management_request_histo_show(struct device *dev,
+			struct device_attribute *attr, char *page);
+extern ssize_t part_management_dma_histo_show(struct device *dev,
+			struct device_attribute *attr, char *page);
+extern ssize_t part_read_histo_clear(struct device *dev,
+		struct device_attribute *attr, const char *page, size_t count);
+extern ssize_t part_write_histo_clear(struct device *dev,
+		struct device_attribute *attr, const char *page, size_t count);
+extern ssize_t part_seek_histo_clear(struct device *dev,
+		struct device_attribute *attr, const char *page, size_t count);
+extern ssize_t part_management_histo_clear(struct device *dev,
+		struct device_attribute *attr, const char *page, size_t count);
+
+extern void init_part_histo_defaults(struct hd_struct *part);
+
+extern ssize_t part_base_histo_size_show(struct device *dev,
+			struct device_attribute *attr, char *page);
+extern ssize_t part_base_histo_time_show(struct device *dev,
+			struct device_attribute *attr, char *page);
+extern ssize_t part_histo_time_scale_show(struct device *dev,
+			struct device_attribute *attr, char *page);
+extern ssize_t part_base_histo_seek_show(struct device *dev,
+			struct device_attribute *attr, char *page);
+extern ssize_t part_base_histo_size_write(struct device *dev,
+		struct device_attribute *attr, const char *page, size_t count);
+extern ssize_t part_base_histo_time_write(struct device *dev,
+		struct device_attribute *attr, const char *page, size_t count);
+extern ssize_t part_histo_time_scale_write(struct device *dev,
+		struct device_attribute *attr, const char *page, size_t count);
+extern ssize_t part_base_histo_seek_write(struct device *dev,
+		struct device_attribute *attr, const char *page, size_t count);
+extern int stats_time_bucket(unsigned long long nanos, int base_histo_time);
+extern int stats_time_bucket_ns(int ns, int base_histo_time);
+extern int stats_size_bucket(sector_t sectors, int base_histo_size);
+#else
+#define block_histogram_completion(cpu, part, req, now, management)
+#define init_part_histo_defaults(part)
+#endif
+
 /* drivers/char/random.c */
 extern void add_disk_randomness(struct gendisk *disk) __latent_entropy;
 extern void rand_initialize_disk(struct gendisk *disk);
diff --git a/include/linux/time.h b/include/linux/time.h
index 4b62a2c0a661..df6c9626d76d 100644
--- a/include/linux/time.h
+++ b/include/linux/time.h
@@ -18,6 +18,11 @@ int get_itimerspec64(struct itimerspec64 *it,
 int put_itimerspec64(const struct itimerspec64 *it,
 			struct itimerspec __user *uit);
 
+static inline unsigned int nsecs_to_msecs(unsigned long long time)
+{
+	return time / NSEC_PER_MSEC;
+}
+
 extern time64_t mktime64(const unsigned int year, const unsigned int mon,
 			const unsigned int day, const unsigned int hour,
 			const unsigned int min, const unsigned int sec);
-- 
2.28.0.rc0.105.gf9edc3c819-goog

